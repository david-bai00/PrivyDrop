import { CustomFile } from "@/types/webrtc";
import { TransferConfig } from "./TransferConfig";
import { postLogToBackend } from "@/app/config/api";

/**
 * ğŸš€ ç½‘ç»œå—ä¿¡æ¯æ¥å£
 */
export interface NetworkChunk {
  chunk: ArrayBuffer | null;
  chunkIndex: number;
  totalChunks: number;
  fileOffset: number;
  isLastChunk: boolean;
}

/**
 * ğŸš€ é«˜æ€§èƒ½æµå¼æ–‡ä»¶è¯»å–å™¨
 * ä½¿ç”¨åŒå±‚ç¼“å†²æ¶æ„ï¼šå¤§å—æ‰¹é‡è¯»å– + å°å—ç½‘ç»œå‘é€
 * è§£å†³æ–‡ä»¶è¯»å–æ€§èƒ½ç“¶é¢ˆé—®é¢˜
 */
export class StreamingFileReader {
  // é…ç½®å‚æ•°
  private readonly BATCH_SIZE =
    TransferConfig.FILE_CONFIG.CHUNK_SIZE *
    TransferConfig.FILE_CONFIG.BATCH_SIZE; // 32MBæ‰¹æ¬¡
  private readonly NETWORK_CHUNK_SIZE =
    TransferConfig.FILE_CONFIG.NETWORK_CHUNK_SIZE; // 64KBç½‘ç»œå—
  private readonly CHUNKS_PER_BATCH = this.BATCH_SIZE / this.NETWORK_CHUNK_SIZE; // 512å—

  // æ–‡ä»¶çŠ¶æ€
  private file: File;
  private fileReader: FileReader;
  private totalFileSize: number;

  // æ‰¹æ¬¡ç¼“å†²çŠ¶æ€
  private currentBatch: ArrayBuffer | null = null; // å½“å‰32MBæ‰¹æ¬¡æ•°æ®
  private currentBatchStartOffset = 0; // å½“å‰æ‰¹æ¬¡åœ¨æ–‡ä»¶ä¸­çš„èµ·å§‹ä½ç½®
  private currentChunkIndexInBatch = 0; // å½“å‰ç½‘ç»œå—åœ¨æ‰¹æ¬¡ä¸­çš„ç´¢å¼•

  // å…¨å±€çŠ¶æ€
  private totalFileOffset = 0; // å½“å‰åœ¨æ•´ä¸ªæ–‡ä»¶ä¸­çš„ä½ç½®
  private isFinished = false;
  private isReading = false; // é˜²æ­¢å¹¶å‘è¯»å–

  constructor(file: CustomFile, startOffset: number = 0) {
    this.file = file;
    this.totalFileSize = file.size;
    this.totalFileOffset = startOffset;
    this.fileReader = new FileReader();

    postLogToBackend(
      `[DEBUG] ğŸ“– StreamingFileReader created - file: ${file.name}, size: ${this.totalFileSize}, startOffset: ${startOffset}`
    );
  }

  /**
   * ğŸ¯ æ ¸å¿ƒæ–¹æ³•ï¼šè·å–ä¸‹ä¸€ä¸ª64KBç½‘ç»œå—
   */
  async getNextNetworkChunk(): Promise<NetworkChunk> {
    // 1. æ£€æŸ¥æ˜¯å¦éœ€è¦åŠ è½½æ–°æ‰¹æ¬¡
    if (this.needsNewBatch()) {
      await this.loadNextBatch();
    }

    // 2. æ£€æŸ¥æ˜¯å¦å·²åˆ°æ–‡ä»¶æœ«å°¾
    if (this.isFinished || !this.currentBatch) {
      return {
        chunk: null,
        chunkIndex: this.calculateGlobalChunkIndex(),
        totalChunks: this.calculateTotalNetworkChunks(),
        fileOffset: this.totalFileOffset,
        isLastChunk: true,
      };
    }

    // 3. ä»å½“å‰æ‰¹æ¬¡ä¸­åˆ‡ç‰‡å‡º64KBç½‘ç»œå—
    const networkChunk = this.sliceNetworkChunkFromBatch();
    const globalChunkIndex = this.calculateGlobalChunkIndex();
    const isLast = this.isLastNetworkChunk(networkChunk);

    // 4. æ›´æ–°çŠ¶æ€
    this.updateChunkState(networkChunk);

    // åªåœ¨å…³é”®èŠ‚ç‚¹è¾“å‡ºæ—¥å¿—
    if (globalChunkIndex % 100 === 0 || isLast) {
      postLogToBackend(
        `[PERF] âœ‚ï¸ CHUNK progress #${globalChunkIndex}/${this.calculateTotalNetworkChunks()} - size: ${
          networkChunk.byteLength
        }, isLast: ${isLast}`
      );
    }

    return {
      chunk: networkChunk,
      chunkIndex: globalChunkIndex,
      totalChunks: this.calculateTotalNetworkChunks(),
      fileOffset: this.totalFileOffset - networkChunk.byteLength,
      isLastChunk: isLast,
    };
  }

  /**
   * ğŸ” åˆ¤æ–­æ˜¯å¦éœ€è¦åŠ è½½æ–°æ‰¹æ¬¡
   */
  private needsNewBatch(): boolean {
    return (
      this.currentBatch === null || // è¿˜æœªåŠ è½½ä»»ä½•æ‰¹æ¬¡
      this.currentChunkIndexInBatch >= this.CHUNKS_PER_BATCH || // å½“å‰æ‰¹æ¬¡ç”¨å®Œ
      this.isCurrentBatchEmpty() // å½“å‰æ‰¹æ¬¡å·²æ— æ•°æ®
    );
  }

  /**
   * ğŸ” åˆ¤æ–­å½“å‰æ‰¹æ¬¡æ˜¯å¦ä¸ºç©º
   */
  private isCurrentBatchEmpty(): boolean {
    if (!this.currentBatch) return true;

    const usedBytes = this.currentChunkIndexInBatch * this.NETWORK_CHUNK_SIZE;
    return usedBytes >= this.currentBatch.byteLength;
  }

  /**
   * ğŸ“¥ åŠ è½½ä¸‹ä¸€ä¸ª32MBæ‰¹æ¬¡åˆ°å†…å­˜
   */
  private async loadNextBatch(): Promise<void> {
    if (this.isReading) {
      // é˜²æ­¢å¹¶å‘è¯»å–
      while (this.isReading) {
        await new Promise((resolve) => setTimeout(resolve, 10));
      }
      return;
    }

    this.isReading = true;
    const startTime = performance.now();

    try {
      // 1. æ¸…ç†æ—§æ‰¹æ¬¡å†…å­˜
      this.currentBatch = null;

      // 2. è®¡ç®—æœ¬æ¬¡è¦è¯»å–çš„å¤§å°
      const remainingFileSize = this.totalFileSize - this.totalFileOffset;
      const batchSize = Math.min(this.BATCH_SIZE, remainingFileSize);

      if (batchSize <= 0) {
        this.isFinished = true;
        return;
      }

      // 3. æ‰§è¡Œå¤§å—æ–‡ä»¶è¯»å–
      const sliceStartTime = performance.now();
      const fileSlice = this.file.slice(
        this.totalFileOffset,
        this.totalFileOffset + batchSize
      );
      const sliceTime = performance.now() - sliceStartTime;

      // 4. å¼‚æ­¥è¯»å–æ–‡ä»¶æ•°æ®
      const readStartTime = performance.now();
      this.currentBatch = await this.readFileSlice(fileSlice);
      const readTime = performance.now() - readStartTime;

      this.currentBatchStartOffset = this.totalFileOffset;
      this.currentChunkIndexInBatch = 0;

      const totalTime = performance.now() - startTime;
      const speedMBps = batchSize / 1024 / 1024 / (totalTime / 1000);

      postLogToBackend(
        `[PERF] ğŸ“– BATCH_READ - size: ${(batchSize / 1024 / 1024).toFixed(
          1
        )}MB, total: ${totalTime.toFixed(1)}ms, slice: ${sliceTime.toFixed(
          1
        )}ms, read: ${readTime.toFixed(1)}ms, speed: ${speedMBps.toFixed(
          1
        )}MB/s`
      );
    } catch (error) {
      postLogToBackend(
        `[PERF] âŒ BATCH_READ failed after ${(
          performance.now() - startTime
        ).toFixed(1)}ms: ${error}`
      );
      throw new Error(`Failed to load file batch: ${error}`);
    } finally {
      this.isReading = false;
    }
  }

  /**
   * ğŸ“„ æ‰§è¡Œæ–‡ä»¶è¯»å–æ“ä½œ
   */
  private async readFileSlice(fileSlice: Blob): Promise<ArrayBuffer> {
    return new Promise((resolve, reject) => {
      this.fileReader.onload = () => {
        const result = this.fileReader.result as ArrayBuffer;
        if (result) {
          resolve(result);
        } else {
          reject(new Error("FileReader result is null"));
        }
      };

      this.fileReader.onerror = () => {
        reject(
          new Error(
            `File reading failed: ${
              this.fileReader.error?.message || "Unknown error"
            }`
          )
        );
      };

      this.fileReader.readAsArrayBuffer(fileSlice);
    });
  }

  /**
   * âœ‚ï¸ ä»32MBæ‰¹æ¬¡ä¸­åˆ‡ç‰‡å‡º64KBç½‘ç»œå—
   */
  private sliceNetworkChunkFromBatch(): ArrayBuffer {
    if (!this.currentBatch) {
      throw new Error("No current batch available for slicing");
    }

    const chunkStartInBatch =
      this.currentChunkIndexInBatch * this.NETWORK_CHUNK_SIZE;
    const remainingInBatch = this.currentBatch.byteLength - chunkStartInBatch;
    const chunkSize = Math.min(this.NETWORK_CHUNK_SIZE, remainingInBatch);

    if (chunkSize <= 0) {
      throw new Error("Invalid chunk size calculated");
    }

    const networkChunk = this.currentBatch.slice(
      chunkStartInBatch,
      chunkStartInBatch + chunkSize
    );

    // åˆ é™¤é¢‘ç¹çš„sliceæ—¥å¿—ï¼Œåªåœ¨éœ€è¦æ—¶è¾“å‡º
    return networkChunk;
  }

  /**
   * ğŸ“Š è®¡ç®—å…¨å±€ç½‘ç»œå—ç´¢å¼•
   */
  private calculateGlobalChunkIndex(): number {
    const batchesBefore = Math.floor(
      this.currentBatchStartOffset / this.BATCH_SIZE
    );
    const chunksInPreviousBatches = batchesBefore * this.CHUNKS_PER_BATCH;
    return chunksInPreviousBatches + this.currentChunkIndexInBatch;
  }

  /**
   * ğŸ“ˆ è®¡ç®—æ€»ç½‘ç»œå—æ•°é‡
   */
  private calculateTotalNetworkChunks(): number {
    return Math.ceil(this.totalFileSize / this.NETWORK_CHUNK_SIZE);
  }

  /**
   * â­ï¸ æ›´æ–°å½“å‰å¤„ç†çŠ¶æ€
   */
  private updateChunkState(chunk: ArrayBuffer): void {
    this.currentChunkIndexInBatch++;
    this.totalFileOffset += chunk.byteLength;

    // æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æ–‡ä»¶æœ«å°¾
    if (this.totalFileOffset >= this.totalFileSize) {
      this.isFinished = true;
      postLogToBackend(
        `[DEBUG] ğŸ File reading completed - totalOffset: ${this.totalFileOffset}, fileSize: ${this.totalFileSize}`
      );
    }
  }

  /**
   * ğŸ åˆ¤æ–­æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªç½‘ç»œå—
   */
  private isLastNetworkChunk(chunk: ArrayBuffer): boolean {
    return this.totalFileOffset + chunk.byteLength >= this.totalFileSize;
  }

  /**
   * ğŸ“Š è·å–è¯»å–è¿›åº¦ä¿¡æ¯
   */
  public getProgress(): {
    readBytes: number;
    totalBytes: number;
    progressPercent: number;
    currentBatchInfo?: {
      batchStartOffset: number;
      batchSize: number;
      chunkIndex: number;
      totalChunks: number;
    };
  } {
    const progressPercent =
      this.totalFileSize > 0
        ? (this.totalFileOffset / this.totalFileSize) * 100
        : 0;

    const result = {
      readBytes: this.totalFileOffset,
      totalBytes: this.totalFileSize,
      progressPercent,
    } as any;

    if (this.currentBatch) {
      result.currentBatchInfo = {
        batchStartOffset: this.currentBatchStartOffset,
        batchSize: this.currentBatch.byteLength,
        chunkIndex: this.currentChunkIndexInBatch,
        totalChunks: Math.ceil(
          this.currentBatch.byteLength / this.NETWORK_CHUNK_SIZE
        ),
      };
    }

    return result;
  }

  /**
   * ğŸ”„ é‡ç½®è¯»å–å™¨çŠ¶æ€ï¼ˆç”¨äºé‡æ–°å¼€å§‹è¯»å–ï¼‰
   */
  public reset(startOffset: number = 0): void {
    this.totalFileOffset = startOffset;
    this.isFinished = false;
    this.isReading = false;
    this.currentBatch = null;
    this.currentBatchStartOffset = 0;
    this.currentChunkIndexInBatch = 0;

    postLogToBackend(
      `[DEBUG] ğŸ”„ StreamingFileReader reset - startOffset: ${startOffset}`
    );
  }

  /**
   * ğŸ§¹ æ¸…ç†å’Œé‡Šæ”¾èµ„æº
   */
  public cleanup(): void {
    // ä¸­æ–­æ­£åœ¨è¿›è¡Œçš„æ–‡ä»¶è¯»å–
    if (this.isReading) {
      this.fileReader.abort();
    }

    // æ¸…ç†å†…å­˜
    this.currentBatch = null;
    this.isFinished = true;
    this.isReading = false;

    postLogToBackend(
      `[DEBUG] ğŸ§¹ StreamingFileReader cleaned up - file: ${this.file.name}`
    );
  }

  /**
   * ğŸ” è·å–è°ƒè¯•ä¿¡æ¯
   */
  public getDebugInfo() {
    return {
      fileName: this.file.name,
      fileSize: this.totalFileSize,
      currentOffset: this.totalFileOffset,
      isFinished: this.isFinished,
      isReading: this.isReading,
      hasBatch: !!this.currentBatch,
      batchOffset: this.currentBatchStartOffset,
      chunkInBatch: this.currentChunkIndexInBatch,
      globalChunkIndex: this.calculateGlobalChunkIndex(),
      totalChunks: this.calculateTotalNetworkChunks(),
    };
  }
}
